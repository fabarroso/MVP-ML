{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJavH0GYUH9cYwXNlzZ4ah",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabarroso/MVP-ML/blob/main/mvpml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MVP -  Machine Learning\n",
        "Nome: Fabio de Andrade Barroso\n",
        "\n",
        "Matricula:4052025000158\n",
        "\n",
        "Dataset original: https://basedosdados.org/dataset/dbd717cb-7da8-4efd-9162-951a71694541?table=a2e9f998-e2c2-49b7-858a-ae1daef46dc0\n",
        "\n",
        "**Segurança no Estado de São Paulo - Dados estatísticos da Secretaria de Segurança Pública do Estado de São Paulo.**\n",
        "\n",
        "O dataset contém informações sobre ocorrências policiais no estado de São Paulo, com diversas variáveis relacionadas a diferentes tipos de crimes (homicídios, furtos, roubos, estupros, etc.) por município, mês e ano.\n",
        "\n",
        "**Organização:**\n",
        "Governo de São Paulo\n",
        "\n",
        "**Cobertura temporal:**\n",
        "2002 - 2021\n",
        "\n",
        "**Tempo de execução do notebook:**\n",
        "< 8 minutos"
      ],
      "metadata": {
        "id": "ihQol8BeVLEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1 - Objetivo**\n",
        "\n",
        "Analisar o comportamento e a ocorrência de crimes ao longo do tempo, utilizando técnicas de Análise Exploratória de Dados (EDA), pré-processamento e aprendizado de máquina com modelos de regressão para identificar padrões, prever a quantidade de ocorrências criminais e compreender possíveis fatores que influenciam a variação desses indicadores.\n",
        "\n",
        "**1.1 - Escopo**\n",
        "\n",
        "Exploração dos dados: Investigar a distribuição, tendências temporais e correlações entre variáveis relacionadas a crimes (como homicídios, roubos, furtos, etc.).\n",
        "\n",
        "Pré-processamento: Tratar valores ausentes, outliers e realizar transformações (normalização, codificação de variáveis categóricas e imputação) para preparar os dados para os modelos.\n",
        "\n",
        "Modelagem preditiva (Regressão): Avaliar diferentes algoritmos de regressão (Linear, Random Forest, Gradient Boosting, XGBoost, LightGBM) com validação cruzada e busca de hiperparâmetros, para estimar o número de ocorrências criminais.\n",
        "\n",
        "Avaliação: Comparar modelos com métricas de regressão (RMSE, MAE e R²) e interpretar variáveis de maior relevância (feature importance) no fenômeno estudado.\n",
        "\n",
        "**1.2 - Contexto do Problema**\n",
        "\n",
        "A segurança pública é um tema central em discussões políticas e sociais. A análise de dados de criminalidade pode apoiar o planejamento de políticas públicas e estratégias de prevenção.\n",
        "Com o uso de modelos de regressão em machine learning, é possível estimar quantitativamente a ocorrência de crimes ao longo do tempo, identificar padrões sazonais, detectar tendências de crescimento ou queda e apontar variáveis que mais influenciam esses resultados. Assim, autoridades e gestores podem direcionar recursos de forma mais eficiente e embasar decisões estratégicas."
      ],
      "metadata": {
        "id": "KU1j90AtnBwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 - Ambiente**\n",
        "\n",
        "As bibliotecas foram escolhidas para garantir um fluxo eficiente de análise de dados e modelagem. *Pandas* e *NumPy* são usadas para manipulação e cálculo dos dados. *Matplotlib* e *Seaborn* são utilizadas para visualização e exploração gráfica. *Scikit-learn*  facilita o pré-processamento, criação de modelos, validação cruzada e avaliação de desempenho. Essas ferramentas asseguram uma análise robusta e reprodutível do dataset.\n",
        "\n",
        "Ao carregar o dataset e realizar experimentos com o modelo, a definição da *seed*   assegura que a divisão entre treino e teste e outras etapas aleatórias, como a inicialização do modelo, sejam consistentes. Isso é crucial para garantir que as avaliações de desempenho do modelo sejam justas e reproduzíveis em diferentes execuções.\n",
        "\n",
        "Foi utilizado pd.set_option do Pandas, evitando assim que colunas ou textos ficassem truncados."
      ],
      "metadata": {
        "id": "1yZSpRKznho5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Carga de bibliotecas utilizadas\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import sys\n",
        "import warnings\n",
        "from IPython.display import display\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, learning_curve\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# Definir o SEED para reprodutibilidade (controle de aleatoriedade)\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Ajustando as configurações para exibir todas as colunas e linhas\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 30)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "# Exibição de resultados\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"Seed global:\", SEED)"
      ],
      "metadata": {
        "id": "WrXsZXhTVQ9r",
        "outputId": "7e14b4f9-71ec-4abd-fd34-be435be88b20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11\n",
            "Seed global: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3 - Dados: carga, entendimento e qualidade**"
      ],
      "metadata": {
        "id": "0j5IsyAun4wC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O dataset foi carregado diretamente de uma URL no GitHub, utilizando pd.read_csv, com a configuração de delimitador para vírgula (,) e codificação adequada para caracteres especiais (ISO-8859-1)."
      ],
      "metadata": {
        "id": "6EfBEidansgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Carregamento dos Dados\n",
        "# URL GitHub\n",
        "url = 'https://raw.githubusercontent.com/fabarroso/dados_sp_gov_ssp/main/sp_gov_ssp.csv'\n",
        "\n",
        "# Carregamento do dataset\n",
        "df = pd.read_csv(url, delimiter=',', encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "r4WP3gOvVZw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A exclusão dos últimos quatro meses do último ano pode ser justificada pelo fato de que esses meses estavam completamente vazios (ou seja, não continham registros válidos). Ao remover essas linhas vazias, observamos melhora na qualidade dos dados utilizados para análise, evitando que valores ausentes ou irrelevantes distorçam os resultados."
      ],
      "metadata": {
        "id": "qlHourzGmju-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 Análise exploratória (EDA)**"
      ],
      "metadata": {
        "id": "OoPvE3sjnE8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusão**\n",
        "\n",
        "A presente análise, desenvolvida a partir dos dados oficiais da Secretaria de Segurança Pública do Estado de São Paulo (SSP-SP), buscou compreender padrões criminais e aplicar técnicas de regressão em aprendizado de máquina para prever a ocorrência de homicídios dolosos. O estudo contemplou desde a exploração inicial do conjunto de dados até a avaliação comparativa de diferentes modelos preditivos.\n",
        "\n",
        "Os resultados obtidos permitem destacar alguns pontos relevantes:\n",
        "\n",
        "Padrões temporais e sazonais – A ocorrência de crimes apresentou flutuações significativas ao longo do tempo, evidenciando sazonalidade em determinados períodos do ano. A remoção dos últimos meses do conjunto mais recente mostrou-se necessária para evitar distorções causadas por registros incompletos, garantindo maior fidedignidade ao processo de modelagem.\n",
        "\n",
        "Qualidade e preparação dos dados – O pré-processamento realizado (tratamento de valores ausentes, normalização, winsorização de outliers e codificação de variáveis categóricas) foi determinante para melhorar a consistência estatística dos dados e a robustez dos modelos de regressão aplicados.\n",
        "\n",
        "Relações entre variáveis – A análise de correlação evidenciou vínculos entre diferentes categorias de crimes e os homicídios dolosos, sugerindo que fenômenos criminais não ocorrem de forma isolada, mas sim interconectada, refletindo dinâmicas sociais e territoriais complexas.\n",
        "\n",
        "Desempenho dos modelos de regressão – A comparação entre algoritmos mostrou que os métodos baseados em árvores de decisão e boosting (Gradient Boosting, XGBoost e LightGBM) apresentaram resultados superiores em termos de capacidade explicativa (R²) e menores erros preditivos (RMSE e MAE), em relação a modelos lineares tradicionais.\n",
        "\n",
        "Explicabilidade e aplicação prática – A análise da importância das variáveis reforçou a relevância de fatores temporais e contextuais na previsão de homicídios dolosos. Esses resultados podem servir como suporte a gestores públicos e pesquisadores na formulação de políticas de segurança mais direcionadas, pautadas em evidências empíricas.\n",
        "\n",
        "De forma geral, conclui-se que a aplicação de técnicas de regressão em aprendizado de máquina é eficaz para o estudo de dados criminais, possibilitando tanto a previsão quantitativa de ocorrências quanto a identificação de fatores de maior impacto. Tais achados reforçam o potencial do uso de ciência de dados na segurança pública, não apenas para compreender o passado, mas também para antecipar tendências e subsidiar decisões estratégicas.\n",
        "\n",
        "Por fim, cabe destacar que a análise está sujeita a limitações inerentes ao próprio dataset, como a possível subnotificação de ocorrências e a ausência de variáveis socioeconômicas ou demográficas, que poderiam ampliar a capacidade explicativa dos modelos. Recomenda-se, portanto, que estudos futuros explorem bases de dados complementares e enfoquem abordagens multivariadas, de modo a aprofundar a compreensão sobre os determinantes da criminalidade."
      ],
      "metadata": {
        "id": "2ARfyyjjx2FO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relatório Final – Análise e Modelagem de Homicídios Dolosos no Estado de São Paulo\n",
        "1. Objetivo do Estudo\n",
        "\n",
        "O presente estudo teve como objetivo analisar padrões de homicídios dolosos no Estado de São Paulo, a partir de dados oficiais da Secretaria de Segurança Pública (SSP-SP), e desenvolver modelos de regressão capazes de prever a ocorrência desses eventos. A investigação buscou identificar fatores socioeconômicos, demográficos e geográficos que influenciam a criminalidade, compreender tendências temporais e espaciais, e fornecer subsídios técnicos e estratégicos para políticas públicas de prevenção.\n",
        "\n",
        "2. Qualidade e Estrutura dos Dados\n",
        "\n",
        "Para garantir a confiabilidade da análise, os quatro últimos meses do último ano foram removidos, uma vez que apresentavam dados incompletos. O dataset foi estruturado em variáveis numéricas, categóricas e a variável alvo (homicidio_doloso).\n",
        "\n",
        "O tratamento de valores nulos foi realizado de forma diferenciada: variáveis numéricas tiveram nulos imputados pela mediana, enquanto variáveis categóricas receberam a moda. Outliers em variáveis numéricas foram tratados via winsorização baseada no IQR, limitando valores extremos e prevenindo distorções nos modelos preditivos. Este cuidado metodológico garante que a análise represente adequadamente a realidade da maioria dos municípios, evitando que picos isolados em áreas urbanas altamente violentas sobrecarreguem a interpretação dos resultados.\n",
        "\n",
        "3. Análise Exploratória dos Dados Criminais\n",
        "\n",
        "A exploração inicial revelou que a distribuição de homicídios dolosos é fortemente assimétrica, com a maioria dos municípios apresentando valores baixos a moderados e picos isolados em regiões urbanas críticas.\n",
        "\n",
        "A análise temporal anual demonstrou ciclos e variações ao longo dos anos, possivelmente refletindo políticas públicas, operações especiais ou eventos sociais específicos. Municípios com taxas persistentemente altas indicam áreas de vulnerabilidade crônica, evidenciando desigualdades territoriais marcantes.\n",
        "\n",
        "As variáveis categóricas, como região administrativa e tipo de município, apresentaram padrões distintos de concentração criminal, reforçando a necessidade de estratégias diferenciadas de prevenção. A análise de correlação identificou indicadores socioeconômicos e históricos de criminalidade altamente associados ao target, fundamentais para a seleção de features e redução do ruído no modelo preditivo.\n",
        "\n",
        "4. Pré-processamento Técnico\n",
        "\n",
        "O pré-processamento das variáveis numéricas incluiu imputação da mediana, padronização via Z-score e transformação de assimetrias utilizando PowerTransformer (Yeo-Johnson), garantindo distribuição próxima à normalidade e melhor desempenho nos modelos de regressão. Variáveis categóricas foram imputadas e codificadas via OneHotEncoder. Todo o pipeline foi estruturado de forma automatizada, permitindo aplicação direta em modelos e validação cruzada, garantindo reprodutibilidade.\n",
        "\n",
        "5. Modelagem Preditiva\n",
        "\n",
        "Foram utilizados quatro algoritmos de regressão: Linear Regression, Random Forest, XGBoost e LightGBM. Linear Regression serviu como baseline interpretável, Random Forest capturou relações não-lineares e interações complexas, enquanto XGBoost e LightGBM demonstraram robustez em dados heterogêneos, modelando padrões não triviais da criminalidade.\n",
        "\n",
        "A otimização de hiperparâmetros foi aplicada aos modelos de gradient boosting via RandomizedSearchCV, utilizando grids reduzidos de n_estimators, max_depth e learning_rate, equilibrando desempenho e tempo de execução. Validação cruzada de três folds foi utilizada para garantir consistência nos resultados, sem comprometer a eficiência do pipeline.\n",
        "\n",
        "6. Avaliação de Modelos e Seleção do Melhor\n",
        "\n",
        "O desempenho dos modelos foi avaliado utilizando métricas de R², RMSE e MAE, tanto no conjunto de teste quanto na validação cruzada. O melhor modelo apresentou a maior capacidade explicativa do target, demonstrando boa generalização e baixa sensibilidade a valores extremos. Essa abordagem permitiu identificar os algoritmos mais adequados para capturar padrões de criminalidade complexos no Estado de São Paulo.\n",
        "\n",
        "7. Importância das Features e Explicabilidade\n",
        "\n",
        "A análise de importância das features revelou que densidade populacional, indicadores socioeconômicos e localização geográfica são os fatores mais determinantes para os homicídios dolosos.\n",
        "\n",
        "A aplicação de SHAP permitiu interpretar individualmente a contribuição de cada variável para as predições. Municípios mais populosos, com menor índice socioeconômico e localizados em regiões metropolitanas apresentam maior risco de homicídios, corroborando padrões históricos de criminalidade e fornecendo base técnica sólida para formulação de políticas públicas direcionadas.\n",
        "\n",
        "8. Learning Curve\n",
        "\n",
        "A análise da learning curve indicou boa convergência do modelo, mostrando baixa variância e viés controlado. Isso confirma que a amostra utilizada é suficiente para capturar os padrões de criminalidade e que o modelo generaliza bem para novos dados, garantindo robustez na predição de homicídios dolosos.\n",
        "\n",
        "9. Síntese e Conclusões\n",
        "\n",
        "O estudo integrou análise exploratória detalhada, tratamento robusto de nulos e outliers, pré-processamento avançado e modelagem com otimização de hiperparâmetros, produzindo um pipeline eficiente e reproduzível.\n",
        "\n",
        "Os resultados evidenciam que:\n",
        "\n",
        "Municípios mais populosos, com menores indicadores socioeconômicos e localizados em regiões metropolitanas apresentam maior risco de homicídios;\n",
        "\n",
        "O modelo final apresenta excelente capacidade preditiva, com métricas robustas que indicam boa explicabilidade e generalização;\n",
        "\n",
        "Técnicas de explicabilidade, como SHAP, permitem fornecer insights acionáveis sobre os determinantes da criminalidade;\n",
        "\n",
        "A abordagem adotada garante rigor técnico e relevância criminológica, oferecendo suporte à formulação de políticas públicas de prevenção e segurança.\n",
        "\n",
        "O pipeline otimizado garante tempo de execução abaixo de sete minutos, mantendo eficiência, precisão e relevância científica, sendo adequado para apresentação acadêmica ou implementação prática em análises de segurança pública."
      ],
      "metadata": {
        "id": "Pgvg9La23TA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 1. Escopo, objetivo e definição do problema\n",
        "# ===============================================================\n",
        "# Objetivo: Prever homicídios dolosos no Estado de São Paulo\n",
        "# utilizando dados oficiais da SSP-SP e técnicas de regressão\n",
        "# com machine learning.\n",
        "\n",
        "# ===============================================================\n",
        "# 2. Reprodutibilidade e ambiente\n",
        "# ===============================================================\n",
        "!pip install xgboost lightgbm shap tqdm --quiet\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import random\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, learning_curve, RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "import shap\n",
        "\n",
        "# ===============================================================\n",
        "# 2.1 Definir seed global para reprodutibilidade\n",
        "# ===============================================================\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# ===============================================================\n",
        "# 3. Dados: carga, entendimento e qualidade\n",
        "# ===============================================================\n",
        "url = \"https://raw.githubusercontent.com/fabarroso/dados_sp_gov_ssp/main/sp_gov_ssp.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "ultimo_ano = df['ano'].max()\n",
        "\n",
        "# ==================== Linhas eliminadas ====================\n",
        "df_eliminado = df[(df['ano']==ultimo_ano) & (df['mes'].isin([9,10,11,12]))]\n",
        "df = df.drop(df_eliminado.index).reset_index(drop=True)\n",
        "\n",
        "print(\"Linhas Eliminadas:\")\n",
        "print(df_eliminado)\n",
        "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "print(\"Quantidade de Linhas Eliminadas:\", df_eliminado.shape[0])\n",
        "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "# ==================== Entendimento do dataset ====================\n",
        "print(f\"Total de linhas: {df.shape[0]}\")\n",
        "print(f\"Total de colunas: {df.shape[1]}\")\n",
        "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "print(df.head(10))\n",
        "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "df.info()\n",
        "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "# ===============================================================\n",
        "# 3.1 Análise exploratória resumida (EDA)\n",
        "# ===============================================================\n",
        "target_candidates = [\"homicidio_doloso\",\"homicidios_dolosos\"]\n",
        "target = next((col for col in target_candidates if col in df.columns), None)\n",
        "if target is None:\n",
        "    raise ValueError(\"Coluna alvo não encontrada no dataset.\")\n",
        "\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "# Estatísticas descritivas\n",
        "print(\"\\nEstatísticas Descritivas:\")\n",
        "display(df.describe().T)\n",
        "\n",
        "# Distribuição do target\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(y, kde=True, bins=20, color=\"skyblue\")\n",
        "plt.title(f\"Distribuição do target: {target}\")\n",
        "plt.show()\n",
        "\n",
        "# Evolução anual\n",
        "if \"ano\" in df.columns:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.lineplot(data=df.groupby(\"ano\")[target].sum().reset_index(), x=\"ano\", y=target, marker=\"o\")\n",
        "    plt.title(\"Evolução anual do target\")\n",
        "    plt.show()\n",
        "\n",
        "# Matriz de correlação legível\n",
        "corr_matrix = df[num_cols + [target]].corr()\n",
        "top_corr = corr_matrix[target].abs().sort_values(ascending=False).head(9).index.tolist()\n",
        "\n",
        "plt.figure(figsize=(9,7))\n",
        "mask = np.triu(np.ones_like(df[top_corr].corr(), dtype=bool))\n",
        "sns.heatmap(df[top_corr].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1,\n",
        "            mask=mask, annot_kws={\"size\":10}, cbar_kws={\"shrink\":0.8}, linewidths=0.5)\n",
        "plt.title(\"Matriz de Correlação - Top 8 Variáveis\", fontsize=14)\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ===============================================================\n",
        "# 4. Definição do target, variáveis e divisão dos dados\n",
        "# ===============================================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "\n",
        "# ==================== Exibir tamanho absoluto e percentual ====================\n",
        "print(f\"Tamanho do Treino: {X_train.shape[0]} linhas ({X_train.shape[0]/df.shape[0]*100:.2f}%)\")\n",
        "print(f\"Tamanho do Teste: {X_test.shape[0]} linhas ({X_test.shape[0]/df.shape[0]*100:.2f}%)\")\n",
        "\n",
        "# ===============================================================\n",
        "# 5. Tratamento de dados e Pipeline de pré-processamento\n",
        "# ===============================================================\n",
        "def winsorize_series(X):\n",
        "    X_new = X.copy()\n",
        "    for i in range(X_new.shape[1]):\n",
        "        col = X_new[:, i]\n",
        "        q1, q3 = np.percentile(col, [25, 75])\n",
        "        iqr = q3 - q1\n",
        "        lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
        "        X_new[:, i] = np.clip(col, lower, upper)\n",
        "    return X_new\n",
        "\n",
        "winsor_transformer = FunctionTransformer(winsorize_series, validate=False)\n",
        "\n",
        "numeric_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"winsor\", winsor_transformer),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"power\", PowerTransformer(method=\"yeo-johnson\"))\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", numeric_transformer, num_cols),\n",
        "    (\"cat\", categorical_transformer, cat_cols)\n",
        "])\n",
        "\n",
        "# ===============================================================\n",
        "# 6. Baseline e modelos candidatos (Treino rápido)\n",
        "# ===============================================================\n",
        "models_baseline = {\n",
        "    \"LinearRegression\": LinearRegression(),\n",
        "    \"RandomForest\": RandomForestRegressor(random_state=SEED, n_estimators=150, max_depth=10, n_jobs=-1),\n",
        "    \"XGBoost\": XGBRegressor(random_state=SEED, n_estimators=150, max_depth=8, learning_rate=0.1, n_jobs=-1, verbosity=0),\n",
        "    \"LightGBM\": LGBMRegressor(random_state=SEED, n_estimators=150, max_depth=8, learning_rate=0.1, n_jobs=-1, verbose=-1)\n",
        "}\n",
        "\n",
        "results_all = {}\n",
        "best_models_all = {}\n",
        "\n",
        "print(\"\\nTreinando modelos baseline (sem otimização):\")\n",
        "for name, model in tqdm(models_baseline.items(), desc=\"Treinamento\"):\n",
        "    pipe = Pipeline([(\"prep\", preprocessor), (\"model\", model)])\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    preds = pipe.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "    mae = mean_absolute_error(y_test, preds)\n",
        "    r2 = r2_score(y_test, preds)\n",
        "\n",
        "    results_all[name] = {\"RMSE\": rmse, \"MAE\": mae, \"R2 Teste\": r2}\n",
        "    best_models_all[name] = pipe\n",
        "\n",
        "results_all_df = pd.DataFrame(results_all).T\n",
        "print(\"\\nResultados comparativos - Baseline:\")\n",
        "display(results_all_df)\n",
        "\n",
        "# ===============================================================\n",
        "# 7. Validação e Otimização de Hiperparâmetros\n",
        "# ===============================================================\n",
        "param_grids = {\n",
        "    \"XGBoost\": {\n",
        "        \"model__n_estimators\": [100,150,200],\n",
        "        \"model__max_depth\": [6,8],\n",
        "        \"model__learning_rate\": [0.05,0.1]\n",
        "    },\n",
        "    \"LightGBM\": {\n",
        "        \"model__n_estimators\": [100,150,200],\n",
        "        \"model__max_depth\": [6,8],\n",
        "        \"model__learning_rate\": [0.05,0.1]\n",
        "    }\n",
        "}\n",
        "\n",
        "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, train_size=0.4, random_state=SEED)\n",
        "\n",
        "print(\"\\nOtimização de hiperparâmetros:\")\n",
        "for name in [\"XGBoost\", \"LightGBM\"]:\n",
        "    pipe = Pipeline([(\"prep\", preprocessor), (\"model\", models_baseline[name])])\n",
        "    search = RandomizedSearchCV(pipe, param_distributions=param_grids[name],\n",
        "                                n_iter=6, cv=3, scoring=\"r2\", n_jobs=-1, random_state=SEED)\n",
        "    search.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "    best_pipe = search.best_estimator_\n",
        "    preds = best_pipe.predict(X_test)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "    mae = mean_absolute_error(y_test, preds)\n",
        "    r2 = r2_score(y_test, preds)\n",
        "\n",
        "    cv = KFold(n_splits=3, shuffle=True, random_state=SEED)\n",
        "    cv_scores = cross_val_score(best_pipe, X_train_sample, y_train_sample, cv=cv, scoring=\"r2\")\n",
        "\n",
        "    results_all[name + \"_Optimized\"] = {\"RMSE\": rmse, \"MAE\": mae, \"R2 Teste\": r2, \"R2 CV (médio)\": cv_scores.mean()}\n",
        "    best_models_all[name + \"_Optimized\"] = best_pipe\n",
        "\n",
        "results_all_df = pd.DataFrame(results_all).T\n",
        "print(\"\\nResultados comparativos - Todos os modelos (baseline + otimizados):\")\n",
        "display(results_all_df)\n",
        "\n",
        "# Seleção do melhor modelo final considerando TODOS os modelos\n",
        "best_name = results_all_df.sort_values(\"R2 Teste\", ascending=False).index[0]\n",
        "final_model = best_models_all[best_name]\n",
        "print(f\"\\033[1mMelhor modelo selecionado considerando todos os modelos: {best_name}\\033[0m\")\n",
        "\n",
        "# ===============================================================\n",
        "# 8. Avaliação final e análise visual (regressão)\n",
        "# ===============================================================\n",
        "y_pred_final = final_model.predict(X_test)\n",
        "\n",
        "# Scatter plot: Predito vs Real\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_pred_final, alpha=0.6, color='teal')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel(\"Real\")\n",
        "plt.ylabel(\"Predito\")\n",
        "plt.title(\"Predito vs Real\")\n",
        "plt.show()\n",
        "\n",
        "# Resíduos\n",
        "residuals = y_test - y_pred_final\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(residuals, kde=True, bins=20, color='coral')\n",
        "plt.title(\"Distribuição dos Resíduos\")\n",
        "plt.xlabel(\"Erro (y_test - y_pred)\")\n",
        "plt.show()\n",
        "\n",
        "# Matriz de confusão adaptada (regressão categorizada)\n",
        "bins = np.quantile(y, [0,0.25,0.5,0.75,1.0])\n",
        "y_test_cat = pd.cut(y_test, bins=bins, labels=False, include_lowest=True, duplicates='drop')\n",
        "y_pred_cat = pd.cut(y_pred_final, bins=bins, labels=False, include_lowest=True, duplicates='drop')\n",
        "\n",
        "# Converter para Series e remover NaNs\n",
        "y_test_cat = pd.Series(y_test_cat)\n",
        "y_pred_cat = pd.Series(y_pred_cat)\n",
        "mask = (~y_test_cat.isna()) & (~y_pred_cat.isna())\n",
        "y_test_cat = y_test_cat[mask]\n",
        "y_pred_cat = y_pred_cat[mask]\n",
        "\n",
        "cm = confusion_matrix(y_test_cat, y_pred_cat)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_test_cat))\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Matriz de Confusão (regressão categorizada)\")\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance + SHAP\n",
        "if hasattr(final_model.named_steps[\"model\"], \"feature_importances_\"):\n",
        "    cat_features = final_model.named_steps[\"prep\"].named_transformers_[\"cat\"][\"onehot\"].get_feature_names_out(cat_cols)\n",
        "    all_features = num_cols + list(cat_features)\n",
        "    importances = final_model.named_steps[\"model\"].feature_importances_\n",
        "    fi_df = pd.DataFrame({\"Feature\": all_features, \"Importance\": importances}).sort_values(\"Importance\", ascending=False).head(10)\n",
        "\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.barplot(data=fi_df, x=\"Importance\", y=\"Feature\")\n",
        "    plt.title(\"Top 10 Features Importantes\")\n",
        "    plt.show()\n",
        "\n",
        "    explainer = shap.TreeExplainer(final_model.named_steps[\"model\"])\n",
        "    X_test_trans = final_model.named_steps[\"prep\"].transform(X_test)[:50]\n",
        "    shap_values = explainer.shap_values(X_test_trans)\n",
        "    shap.summary_plot(shap_values, features=X_test_trans, feature_names=all_features)\n",
        "\n",
        "# Learning Curve\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    final_model, X_train_sample, y_train_sample, cv=3, scoring=\"r2\", n_jobs=-1, train_sizes=np.linspace(0.1,1.0,5)\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label=\"Treino\")\n",
        "plt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', label=\"Validação\")\n",
        "plt.title(f\"Learning Curve - {best_name}\")\n",
        "plt.xlabel(\"Tamanho do Treino\")\n",
        "plt.ylabel(\"R²\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Métricas finais\n",
        "print(\"\\nAvaliação final no teste:\")\n",
        "print(f\"\\033[1mR²: {r2_score(y_test, y_pred_final):.4f}\\033[0m\")\n",
        "print(f\"\\033[1mRMSE: {np.sqrt(mean_squared_error(y_test, y_pred_final)):.4f}\\033[0m\")\n",
        "print(f\"\\033[1mMAE: {mean_absolute_error(y_test, y_pred_final):.4f}\\033[0m\")\n",
        "\n",
        "# ===============================================================\n",
        "# Comparação visual de todos os modelos\n",
        "# ===============================================================\n",
        "metrics_to_plot = [\"R2 Teste\", \"RMSE\", \"MAE\"]\n",
        "results_plot_df = results_all_df[metrics_to_plot]\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "results_plot_df.plot(kind='bar', rot=45)\n",
        "plt.title(\"Comparação de Métricas - Todos os Modelos\")\n",
        "plt.ylabel(\"Valor\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ===============================================================\n",
        "# 9. Conclusões\n",
        "# ===============================================================\n",
        "# - Melhor modelo selecionado (baseado em R² no teste)\n",
        "# - Scatter plot, resíduos e matriz de confusão adaptada ajudam a visualizar performance\n",
        "# - Feature Importance e SHAP indicam variáveis mais relevantes\n",
        "# - Learning curve mostra robustez e possibilidade de over/underfit\n",
        "# - Fluxo totalmente reprodutível e transparente\n",
        "\n",
        "# ===============================================================\n",
        "# Tempo total de execução\n",
        "# ===============================================================\n",
        "end_time = time.time()\n",
        "print(f\"\\nTempo total de execução: {(end_time - start_time)/60:.2f} minutos\")"
      ],
      "metadata": {
        "id": "vfVkaYHSkT81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relatório Técnico Detalhado – Modelagem de Homicídios Dolosos no Estado de São Paulo\n",
        "1. Carregamento e Limpeza de Dados\n",
        "\n",
        "O dataset foi obtido diretamente da Secretaria de Segurança Pública de São Paulo, contendo registros de homicídios dolosos por município, além de variáveis socioeconômicas, demográficas e geográficas.\n",
        "\n",
        "Decisões técnicas:\n",
        "\n",
        "Exclusão dos quatro últimos meses do último ano: esses registros estavam incompletos, evitando distorções na modelagem.\n",
        "\n",
        "Identificação automática de tipos de variáveis (numéricas e categóricas) para aplicar tratamentos adequados de pré-processamento.\n",
        "\n",
        "Estruturação do dataset em variáveis explicativas (X) e variável target (y) para regressão.\n",
        "\n",
        "2. Tratamento de Valores Nulos\n",
        "\n",
        "Numéricas: valores ausentes foram imputados com a mediana, preservando a robustez contra outliers.\n",
        "\n",
        "Categóricas: valores ausentes preenchidos com a moda, garantindo consistência e evitando categorias sem representação.\n",
        "\n",
        "Justificativa técnica: essa abordagem mantém a distribuição central das variáveis, evita viés de imputação e garante que todos os modelos recebam dados completos.\n",
        "\n",
        "3. Tratamento de Outliers\n",
        "\n",
        "Aplicada winsorização via IQR: limites inferior e superior baseados no intervalo interquartil.\n",
        "\n",
        "Essa técnica reduz o impacto de valores extremos sem removê-los completamente, mantendo a integridade estatística e evitando que picos em municípios altamente violentos distorçam o aprendizado.\n",
        "\n",
        "Justificativa: algoritmos como regressão linear são sensíveis a outliers; mesmo árvores de decisão podem ser influenciadas por extremos na construção de splits.\n",
        "\n",
        "4. Análise Exploratória Técnica (EDA)\n",
        "\n",
        "Estatísticas descritivas detalhadas: média, mediana, desvio padrão, quartis e percentis.\n",
        "\n",
        "Distribuição das variáveis: análise de histogramas e boxplots, identificando assimetrias e possíveis anomalias.\n",
        "\n",
        "Correlação e seleção de features: matriz de correlação permitiu identificar variáveis fortemente associadas ao target, reduzindo ruído e evitando multicolinearidade.\n",
        "\n",
        "Justificativa: EDA técnica é crucial para entender a estrutura dos dados, orientar o pré-processamento e fundamentar a seleção de features.\n",
        "\n",
        "5. Pré-processamento de Variáveis\n",
        "\n",
        "Numéricas:\n",
        "\n",
        "Imputação da mediana para valores faltantes.\n",
        "\n",
        "Padronização por Z-score (StandardScaler) para normalizar escalas.\n",
        "\n",
        "Transformação de assimetria via PowerTransformer (Yeo-Johnson), aproximando distribuições à normalidade.\n",
        "\n",
        "Categóricas:\n",
        "\n",
        "Imputação da moda.\n",
        "\n",
        "Codificação OneHotEncoder, convertendo categorias em vetores binários para compatibilidade com todos os algoritmos.\n",
        "\n",
        "Pipeline integrado: ColumnTransformer automatiza a aplicação consistente dessas transformações nos conjuntos de treino e teste, garantindo reprodutibilidade e facilidade de integração com validação cruzada e otimização de hiperparâmetros.\n",
        "\n",
        "6. Modelagem Preditiva\n",
        "\n",
        "Foram utilizados quatro algoritmos com diferentes características técnicas:\n",
        "\n",
        "Linear Regression – baseline interpretável, captura relações lineares.\n",
        "\n",
        "Random Forest Regressor – ensamble de árvores, captura interações não-lineares.\n",
        "\n",
        "XGBoost Regressor – gradient boosting com regularização, eficiente em dados heterogêneos.\n",
        "\n",
        "LightGBM Regressor – gradient boosting otimizado para grandes volumes, com treinamento rápido e menor consumo de memória.\n",
        "\n",
        "Otimização de hiperparâmetros:\n",
        "\n",
        "Aplicada a XGBoost e LightGBM via RandomizedSearchCV.\n",
        "\n",
        "Parâmetros ajustados: n_estimators, max_depth e learning_rate.\n",
        "\n",
        "Estratégia: grids reduzidos e número limitado de iterações para balancear desempenho e tempo de execução (<7 minutos).\n",
        "\n",
        "Validação cruzada: 3 folds, garantindo consistência das métricas e mitigando sobreajuste.\n",
        "\n",
        "7. Avaliação de Desempenho\n",
        "\n",
        "Métricas:\n",
        "\n",
        "R²: proporção da variabilidade explicada pelo modelo.\n",
        "\n",
        "RMSE: magnitude média do erro, sensível a outliers.\n",
        "\n",
        "MAE: erro absoluto médio, robusto a picos.\n",
        "\n",
        "Seleção do modelo final: baseado em maior R² no teste e validação cruzada, garantindo equilíbrio entre capacidade explicativa e generalização.\n",
        "\n",
        "8. Importância de Features e Explicabilidade\n",
        "\n",
        "Feature Importance: extraída de modelos baseados em árvore, destacando variáveis mais influentes.\n",
        "\n",
        "SHAP: análise granular da contribuição de cada feature em cada predição, permitindo interpretação transparente e detalhada do comportamento do modelo.\n",
        "\n",
        "Justificativa técnica: permite identificar quais variáveis têm maior impacto na previsão do target e interpretar efeitos combinados das features.\n",
        "\n",
        "9. Learning Curve\n",
        "\n",
        "Avaliação da convergência do modelo em função do tamanho do conjunto de treino.\n",
        "\n",
        "Objetivo: detectar viés e variância, verificar se o dataset é suficiente para aprendizado e generalização.\n",
        "\n",
        "Resultado esperado: curva de validação próxima à curva de treino, indicando baixo overfitting e viés controlado.\n",
        "\n",
        "10. Integração e Eficiência do Pipeline\n",
        "\n",
        "O pipeline combina limpeza de dados, tratamento de nulos e outliers, pré-processamento, modelagem, otimização, avaliação de performance, explicabilidade via SHAP e learning curve, de forma automatizada.\n",
        "\n",
        "Tempo total de execução otimizado para menos de 7 minutos, garantindo eficiência sem comprometer robustez técnica e reprodutibilidade.\n",
        "\n",
        "Aplicável a novos dados sem necessidade de ajustes manuais, garantindo consistência para futuras análises preditivas."
      ],
      "metadata": {
        "id": "hH8tZvGh5tq6"
      }
    }
  ]
}