{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzfq69xth9PSbgo3W48vQq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabarroso/MVP-ML/blob/main/mvpml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MVP -  Machine Learning\n",
        "Nome: Fabio de Andrade Barroso\n",
        "\n",
        "Matricula:4052025000158\n",
        "\n",
        "Dataset original: https://basedosdados.org/dataset/dbd717cb-7da8-4efd-9162-951a71694541?table=a2e9f998-e2c2-49b7-858a-ae1daef46dc0\n",
        "\n",
        "**Segurança no Estado de São Paulo - Dados estatísticos da Secretaria de Segurança Pública do Estado de São Paulo.**\n",
        "\n",
        "O dataset contém informações sobre ocorrências policiais no estado de São Paulo, com diversas variáveis relacionadas a diferentes tipos de crimes (homicídios, furtos, roubos, estupros, etc.) por município, mês e ano.\n",
        "\n",
        "**Organização:**\n",
        "Governo de São Paulo\n",
        "\n",
        "**Cobertura temporal:**\n",
        "2002 - 2021"
      ],
      "metadata": {
        "id": "ihQol8BeVLEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1 - Objetivo**\n",
        "\n",
        "Analisar o comportamento e a ocorrência de crimes ao longo do tempo, utilizando técnicas de Análise Exploratória de Dados (EDA), pré-processamento e aprendizado de máquina com modelos de regressão para identificar padrões, prever a quantidade de ocorrências criminais e compreender possíveis fatores que influenciam a variação desses indicadores.\n",
        "\n",
        "**1.1 - Escopo**\n",
        "\n",
        "Exploração dos dados: Investigar a distribuição, tendências temporais e correlações entre variáveis relacionadas a crimes (como homicídios, roubos, furtos, etc.).\n",
        "\n",
        "Pré-processamento: Tratar valores ausentes, outliers e realizar transformações (normalização, codificação de variáveis categóricas e imputação) para preparar os dados para os modelos.\n",
        "\n",
        "Modelagem preditiva (Regressão): Avaliar diferentes algoritmos de regressão (Linear, Random Forest, Gradient Boosting, XGBoost, LightGBM) com validação cruzada e busca de hiperparâmetros, para estimar o número de ocorrências criminais.\n",
        "\n",
        "Avaliação: Comparar modelos com métricas de regressão (RMSE, MAE e R²) e interpretar variáveis de maior relevância (feature importance) no fenômeno estudado.\n",
        "\n",
        "**1.2 - Contexto do Problema**\n",
        "\n",
        "A segurança pública é um tema central em discussões políticas e sociais. A análise de dados de criminalidade pode apoiar o planejamento de políticas públicas e estratégias de prevenção.\n",
        "Com o uso de modelos de regressão em machine learning, é possível estimar quantitativamente a ocorrência de crimes ao longo do tempo, identificar padrões sazonais, detectar tendências de crescimento ou queda e apontar variáveis que mais influenciam esses resultados. Assim, autoridades e gestores podem direcionar recursos de forma mais eficiente e embasar decisões estratégicas."
      ],
      "metadata": {
        "id": "KU1j90AtnBwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 - Ambiente**\n",
        "\n",
        "As bibliotecas foram escolhidas para garantir um fluxo eficiente de análise de dados e modelagem. *Pandas* e *NumPy* são usadas para manipulação e cálculo dos dados. *Matplotlib* e *Seaborn* são utilizadas para visualização e exploração gráfica. *Scikit-learn*  facilita o pré-processamento, criação de modelos, validação cruzada e avaliação de desempenho. Essas ferramentas asseguram uma análise robusta e reprodutível do dataset.\n",
        "\n",
        "Ao carregar o dataset e realizar experimentos com o modelo, a definição da *seed*   assegura que a divisão entre treino e teste e outras etapas aleatórias, como a inicialização do modelo, sejam consistentes. Isso é crucial para garantir que as avaliações de desempenho do modelo sejam justas e reproduzíveis em diferentes execuções.\n",
        "\n",
        "Foi utilizado pd.set_option do Pandas, evitando assim que colunas ou textos ficassem truncados."
      ],
      "metadata": {
        "id": "1yZSpRKznho5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Carga de bibliotecas utilizadas\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import random\n",
        "import sys\n",
        "#from scipy.stats import chi2_contingency\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split,cross_val_score,KFold,GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# Definir o SEED para reprodutibilidade (controle de aleatoriedade)\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Ajustando as configurações para exibir todas as colunas e linhas\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 30)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "# Exibição de resultados\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"Seed global:\", SEED)"
      ],
      "metadata": {
        "id": "WrXsZXhTVQ9r",
        "outputId": "6882f289-a501-4ce0-d3a5-e039f1c8093e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11\n",
            "Seed global: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3 - Dados: carga, entendimento e qualidade**"
      ],
      "metadata": {
        "id": "0j5IsyAun4wC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O dataset foi carregado diretamente de uma URL no GitHub, utilizando pd.read_csv, com a configuração de delimitador para vírgula (,) e codificação adequada para caracteres especiais (ISO-8859-1)."
      ],
      "metadata": {
        "id": "6EfBEidansgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Carregamento dos Dados\n",
        "# URL GitHub\n",
        "url = 'https://raw.githubusercontent.com/fabarroso/dados_sp_gov_ssp/main/sp_gov_ssp.csv'\n",
        "\n",
        "# Carregamento do dataset\n",
        "df = pd.read_csv(url, delimiter=',', encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "r4WP3gOvVZw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A exclusão dos últimos quatro meses do último ano pode ser justificada pelo fato de que esses meses estavam completamente vazios (ou seja, não continham registros válidos). Ao remover essas linhas vazias, observamos melhora na qualidade dos dados utilizados para análise, evitando que valores ausentes ou irrelevantes distorçam os resultados."
      ],
      "metadata": {
        "id": "qlHourzGmju-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibe as primeiras 10 linhas\n",
        "print(df.head(10).to_string())\n",
        "\n",
        "print(\"\\n\" + \"-\"*660 + \"\\n\")\n",
        "\n",
        "# Identificar o último ano no dataset\n",
        "ultimo_ano = df['ano'].max()\n",
        "\n",
        "# Identificar os últimos 4 meses desse último ano\n",
        "meses_para_excluir = [9, 10, 11, 12]  # setembro, outubro, novembro, dezembro\n",
        "\n",
        "# Criar um DataFrame com os dados que serão excluídos (últimos 4 meses do último ano)\n",
        "df_eliminado = df[(df['ano'] == ultimo_ano) & (df['mes'].isin(meses_para_excluir))]\n",
        "\n",
        "# Exibir as linhas eliminadas\n",
        "print(\"Linhas Eliminadas:\")\n",
        "print(df_eliminado)\n",
        "\n",
        "print(\"\\n\" + \"-\"*660 + \"\\n\")\n",
        "\n",
        "# Exibir a quantidade de linhas eliminadas\n",
        "print(\"\\nQuantidade de Linhas Eliminadas:\", df_eliminado.shape[0])\n",
        "\n",
        "# Filtrando o DataFrame para excluir os últimos 4 meses do último ano\n",
        "df_filtrado = df[~((df['ano'] == ultimo_ano) & (df['mes'].isin(meses_para_excluir)))]"
      ],
      "metadata": {
        "id": "JzUwzvQoqvPq",
        "outputId": "bc64f015-84ac-48dc-cb5a-be94a23c1a8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ano  mes  id_municipio           regiao_ssp  homicidio_doloso  numero_de_vitimas_em_homicidio_doloso  homicidio_doloso_por_acidente_de_transito  numero_de_vitimas_em_homicidio_doloso_por_acidente_de_transito  homicidio_culposo_por_acidente_de_transito  homicidio_culposo_outros  tentativa_de_homicidio  lesao_corporal_seguida_de_morte  lesao_corporal_dolosa  lesao_corporal_culposa_por_acidente_de_transito  lesao_corporal_culposa_outras  latrocinio  numero_de_vitimas_em_latrocinio  total_de_estupro  estupro  estupro_de_vulneravel  total_de_roubo_outros  roubo_outros  roubo_de_veiculo  roubo_a_banco  roubo_de_carga  furto_outros  furto_de_veiculo\n",
            "0  2002    1       3500105  Presidente Prudente               0.0                                    0.0                                        0.0                                                             0.0                                         0.0                       0.0                     0.0                              0.0                   24.0                                             13.0                            2.0         0.0                              0.0               0.0      NaN                    NaN                    0.0           NaN               0.0            0.0             0.0          21.0               0.0\n",
            "1  2002    2       3500105  Presidente Prudente               0.0                                    0.0                                        0.0                                                             0.0                                         0.0                       0.0                     0.0                              0.0                   24.0                                             15.0                            2.0         0.0                              0.0               0.0      NaN                    NaN                    0.0           NaN               0.0            0.0             0.0          32.0               0.0\n",
            "2  2002    3       3500105  Presidente Prudente               0.0                                    0.0                                        0.0                                                             0.0                                         0.0                       0.0                     1.0                              0.0                   21.0                                             13.0                            5.0         0.0                              0.0               0.0      NaN                    NaN                    0.0           NaN               0.0            0.0             0.0          36.0               0.0\n",
            "3  2002    4       3500105  Presidente Prudente               0.0                                    0.0                                        0.0                                                             0.0                                         0.0                       0.0                     1.0                              0.0                   36.0                                             13.0                            1.0         0.0                              0.0               0.0      NaN                    NaN                    1.0           NaN               0.0            0.0             0.0          45.0               0.0\n",
            "4  2002    5       3500105  Presidente Prudente               0.0                                    0.0                                        0.0                                                             0.0                                         0.0                       0.0                     0.0                              0.0                   27.0                                             12.0                            1.0         0.0                              0.0               0.0      NaN                    NaN                    1.0           NaN               0.0            0.0             0.0          35.0               0.0\n",
            "5  2002    6       3500105  Presidente Prudente               0.0                                    0.0                                        0.0                                                             0.0                                         0.0                       0.0                     1.0                              0.0                   19.0                                             20.0                            3.0         0.0                              0.0               0.0      NaN                    NaN                    1.0           NaN               0.0            0.0             0.0          25.0               0.0\n",
            "6  2002    7       3500105  Presidente Prudente               0.0                                    0.0                                        0.0                                                             0.0                                         0.0                       0.0                     0.0                              0.0                   18.0                                             16.0                            4.0         0.0                              0.0               0.0      NaN                    NaN                    0.0           NaN               0.0            0.0             0.0          28.0               0.0\n",
            "7  2002    8       3500105  Presidente Prudente               0.0                                    0.0                                        0.0                                                             0.0                                         0.0                       0.0                     0.0                              0.0                   36.0                                             12.0                            2.0         1.0                              1.0               0.0      NaN                    NaN                    0.0           NaN               0.0            0.0             0.0          21.0               0.0\n",
            "8  2002    9       3500105  Presidente Prudente               1.0                                    1.0                                        0.0                                                             0.0                                         0.0                       0.0                     0.0                              0.0                   24.0                                             12.0                            4.0         0.0                              0.0               0.0      NaN                    NaN                    0.0           NaN               0.0            0.0             0.0          17.0               0.0\n",
            "9  2002   10       3500105  Presidente Prudente               0.0                                    0.0                                        0.0                                                             0.0                                         1.0                       0.0                     1.0                              0.0                   35.0                                             11.0                            2.0         0.0                              0.0               0.0      NaN                    NaN                    0.0           NaN               0.0            0.0             0.0          27.0               0.0\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Linhas Eliminadas:\n",
            "         ano  mes  id_municipio               regiao_ssp  homicidio_doloso  numero_de_vitimas_em_homicidio_doloso  homicidio_doloso_por_acidente_de_transito  numero_de_vitimas_em_homicidio_doloso_por_acidente_de_transito  homicidio_culposo_por_acidente_de_transito  homicidio_culposo_outros  tentativa_de_homicidio  lesao_corporal_seguida_de_morte  lesao_corporal_dolosa  lesao_corporal_culposa_por_acidente_de_transito  lesao_corporal_culposa_outras  latrocinio  numero_de_vitimas_em_latrocinio  total_de_estupro  estupro  estupro_de_vulneravel  total_de_roubo_outros  roubo_outros  roubo_de_veiculo  roubo_a_banco  roubo_de_carga  furto_outros  furto_de_veiculo\n",
            "123056  2021    9       3500402               Piracicaba               NaN                                    NaN                                        NaN                                                             NaN                                         NaN                       NaN                     NaN                              NaN                    NaN                                              NaN                            NaN         NaN                              NaN               NaN      NaN                    NaN                    NaN           NaN               NaN            NaN             NaN           NaN               NaN\n",
            "123057  2021   10       3500402               Piracicaba               NaN                                    NaN                                        NaN                                                             NaN                                         NaN                       NaN                     NaN                              NaN                    NaN                                              NaN                            NaN         NaN                              NaN               NaN      NaN                    NaN                    NaN           NaN               NaN            NaN             NaN           NaN               NaN\n",
            "123058  2021   11       3500402               Piracicaba               NaN                                    NaN                                        NaN                                                             NaN                                         NaN                       NaN                     NaN                              NaN                    NaN                                              NaN                            NaN         NaN                              NaN               NaN      NaN                    NaN                    NaN           NaN               NaN            NaN             NaN           NaN               NaN\n",
            "123059  2021   12       3500402               Piracicaba               NaN                                    NaN                                        NaN                                                             NaN                                         NaN                       NaN                     NaN                              NaN                    NaN                                              NaN                            NaN         NaN                              NaN               NaN      NaN                    NaN                    NaN           NaN               NaN            NaN             NaN           NaN               NaN\n",
            "123068  2021    9       3500105      Presidente Prudente               NaN                                    NaN                                        NaN                                                             NaN                                         NaN                       NaN                     NaN                              NaN                    NaN                                              NaN                            NaN         NaN                              NaN               NaN      NaN                    NaN                    NaN           NaN               NaN            NaN             NaN           NaN               NaN\n",
            "...      ...  ...           ...                      ...               ...                                    ...                                        ...                                                             ...                                         ...                       ...                     ...                              ...                    ...                                              ...                            ...         ...                              ...               ...      ...                    ...                    ...           ...               ...            ...             ...           ...               ...\n",
            "130775  2021   12       3501400                    Bauru               NaN                                    NaN                                        NaN                                                             NaN                                         NaN                       NaN                     NaN                              NaN                    NaN                                              NaN                            NaN         NaN                              NaN               NaN      NaN                    NaN                    NaN           NaN               NaN            NaN             NaN           NaN               NaN\n",
            "130784  2021    9       3501806  SÃ£o JosÃ© do Rio Preto               NaN                                    NaN                                        NaN                                                             NaN                                         NaN                       NaN                     NaN                              NaN                    NaN                                              NaN                            NaN         NaN                              NaN               NaN      NaN                    NaN                    NaN           NaN               NaN            NaN             NaN           NaN               NaN\n",
            "130785  2021   10       3501806  SÃ£o JosÃ© do Rio Preto               NaN                                    NaN                                        NaN                                                             NaN                                         NaN                       NaN                     NaN                              NaN                    NaN                                              NaN                            NaN         NaN                              NaN               NaN      NaN                    NaN                    NaN           NaN               NaN            NaN             NaN           NaN               NaN\n",
            "130786  2021   11       3501806  SÃ£o JosÃ© do Rio Preto               NaN                                    NaN                                        NaN                                                             NaN                                         NaN                       NaN                     NaN                              NaN                    NaN                                              NaN                            NaN         NaN                              NaN               NaN      NaN                    NaN                    NaN           NaN               NaN            NaN             NaN           NaN               NaN\n",
            "130787  2021   12       3501806  SÃ£o JosÃ© do Rio Preto               NaN                                    NaN                                        NaN                                                             NaN                                         NaN                       NaN                     NaN                              NaN                    NaN                                              NaN                            NaN         NaN                              NaN               NaN      NaN                    NaN                    NaN           NaN               NaN            NaN             NaN           NaN               NaN\n",
            "\n",
            "[2580 rows x 27 columns]\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Quantidade de Linhas Eliminadas: 2580\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 Análise exploratória (EDA)**"
      ],
      "metadata": {
        "id": "OoPvE3sjnE8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4 - Pré Processamento**"
      ],
      "metadata": {
        "id": "fQ9TmysnPOeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusão**\n",
        "\n",
        "A presente análise, desenvolvida a partir dos dados oficiais da Secretaria de Segurança Pública do Estado de São Paulo (SSP-SP), buscou compreender padrões criminais e aplicar técnicas de regressão em aprendizado de máquina para prever a ocorrência de homicídios dolosos. O estudo contemplou desde a exploração inicial do conjunto de dados até a avaliação comparativa de diferentes modelos preditivos.\n",
        "\n",
        "Os resultados obtidos permitem destacar alguns pontos relevantes:\n",
        "\n",
        "Padrões temporais e sazonais – A ocorrência de crimes apresentou flutuações significativas ao longo do tempo, evidenciando sazonalidade em determinados períodos do ano. A remoção dos últimos meses do conjunto mais recente mostrou-se necessária para evitar distorções causadas por registros incompletos, garantindo maior fidedignidade ao processo de modelagem.\n",
        "\n",
        "Qualidade e preparação dos dados – O pré-processamento realizado (tratamento de valores ausentes, normalização, winsorização de outliers e codificação de variáveis categóricas) foi determinante para melhorar a consistência estatística dos dados e a robustez dos modelos de regressão aplicados.\n",
        "\n",
        "Relações entre variáveis – A análise de correlação evidenciou vínculos entre diferentes categorias de crimes e os homicídios dolosos, sugerindo que fenômenos criminais não ocorrem de forma isolada, mas sim interconectada, refletindo dinâmicas sociais e territoriais complexas.\n",
        "\n",
        "Desempenho dos modelos de regressão – A comparação entre algoritmos mostrou que os métodos baseados em árvores de decisão e boosting (Gradient Boosting, XGBoost e LightGBM) apresentaram resultados superiores em termos de capacidade explicativa (R²) e menores erros preditivos (RMSE e MAE), em relação a modelos lineares tradicionais.\n",
        "\n",
        "Explicabilidade e aplicação prática – A análise da importância das variáveis reforçou a relevância de fatores temporais e contextuais na previsão de homicídios dolosos. Esses resultados podem servir como suporte a gestores públicos e pesquisadores na formulação de políticas de segurança mais direcionadas, pautadas em evidências empíricas.\n",
        "\n",
        "De forma geral, conclui-se que a aplicação de técnicas de regressão em aprendizado de máquina é eficaz para o estudo de dados criminais, possibilitando tanto a previsão quantitativa de ocorrências quanto a identificação de fatores de maior impacto. Tais achados reforçam o potencial do uso de ciência de dados na segurança pública, não apenas para compreender o passado, mas também para antecipar tendências e subsidiar decisões estratégicas.\n",
        "\n",
        "Por fim, cabe destacar que a análise está sujeita a limitações inerentes ao próprio dataset, como a possível subnotificação de ocorrências e a ausência de variáveis socioeconômicas ou demográficas, que poderiam ampliar a capacidade explicativa dos modelos. Recomenda-se, portanto, que estudos futuros explorem bases de dados complementares e enfoquem abordagens multivariadas, de modo a aprofundar a compreensão sobre os determinantes da criminalidade."
      ],
      "metadata": {
        "id": "2ARfyyjjx2FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Projeto: Regressão Explicativa - SSP SP (Versão Acadêmica Final)\n",
        "# ==============================================================\n",
        "\n",
        "# 1. Ambiente e Reprodutibilidade\n",
        "!pip install xgboost lightgbm --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, learning_curve\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ==============================================================\n",
        "# 2. Objetivo e Definição do Problema\n",
        "\"\"\"\n",
        "Objetivo: Analisar a evolução da criminalidade no Estado de São Paulo e\n",
        "prever a quantidade de homicídios dolosos utilizando regressão e técnicas\n",
        "de aprendizado de máquina.\n",
        "\n",
        "Justificativa: A predição pode apoiar políticas públicas de segurança,\n",
        "otimização de recursos policiais e planejamento estratégico.\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================\n",
        "# 3. Carregar dataset\n",
        "print(\"Carregando dataset...\")\n",
        "url = 'https://raw.githubusercontent.com/fabarroso/dados_sp_gov_ssp/main/sp_gov_ssp.csv'\n",
        "df = pd.read_csv(url)\n",
        "print(\"Shape original:\", df.shape)\n",
        "display(df.head())\n",
        "\n",
        "# ==============================================================\n",
        "# 3.1 Entendimento e Qualidade dos Dados\n",
        "print(\"\\nResumo das colunas e tipos de dados:\")\n",
        "display(df.info())\n",
        "print(\"\\nValores ausentes por coluna (%):\")\n",
        "display((df.isna().sum()/len(df)*100).sort_values(ascending=False))\n",
        "\n",
        "# ==============================================================\n",
        "# 4. Remover últimos 4 meses do último ano\n",
        "ultimo_ano = df['ano'].max()\n",
        "meses_para_excluir = [9,10,11,12]\n",
        "linhas_remover = df[(df['ano']==ultimo_ano)&(df['mes'].isin(meses_para_excluir))]\n",
        "print(f\"\\nNúmero de linhas a serem removidas: {linhas_remover.shape[0]}\")\n",
        "display(linhas_remover)\n",
        "\n",
        "df = df.drop(linhas_remover.index).reset_index(drop=True)\n",
        "print(\"Shape após remoção últimos 4 meses:\", df.shape)\n",
        "\n",
        "# ==============================================================\n",
        "# 5. Função para Winsorização de outliers\n",
        "def winsorize_outliers(df, num_cols):\n",
        "    df_w = df.copy()\n",
        "    for col in num_cols:\n",
        "        Q1 = df_w[col].quantile(0.25)\n",
        "        Q3 = df_w[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower = Q1 - 1.5*IQR\n",
        "        upper = Q3 + 1.5*IQR\n",
        "        df_w[col] = np.where(df_w[col] < lower, lower, df_w[col])\n",
        "        df_w[col] = np.where(df_w[col] > upper, upper, df_w[col])\n",
        "    return df_w\n",
        "\n",
        "# ==============================================================\n",
        "# 6. EDA completo\n",
        "def eda_regression(df, target):\n",
        "    print(\"\\n=== Estatísticas Descritivas ===\")\n",
        "    display(df.describe().T)\n",
        "\n",
        "    # Histograma do target\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.hist(df[target], bins=20, color='skyblue', edgecolor='black')\n",
        "    plt.title(f'Histograma - {target}')\n",
        "    plt.xlabel(target)\n",
        "    plt.ylabel('Frequência')\n",
        "    plt.show()\n",
        "\n",
        "    # Seleção de variáveis numéricas\n",
        "    num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "    num_cols.remove(target)\n",
        "\n",
        "    # Top 5 variáveis para gráficos\n",
        "    top_num_cols = num_cols[:5]\n",
        "    for col in top_num_cols:\n",
        "        plt.figure(figsize=(6,3))\n",
        "        plt.hist(df[col].dropna(), bins=20, color='lightgreen', edgecolor='black')\n",
        "        plt.title(f'Histograma - {col}')\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(6,3))\n",
        "        sns.violinplot(x=df[col], color='lightblue')\n",
        "        plt.title(f'Violin Plot - {col}')\n",
        "        plt.show()\n",
        "\n",
        "        # Boxplot\n",
        "        plt.figure(figsize=(6,3))\n",
        "        sns.boxplot(x=df[col])\n",
        "        plt.title(f'Boxplot - {col}')\n",
        "        plt.show()\n",
        "\n",
        "    # Heatmap de correlação (Top 10 numéricas mais correlacionadas com o target)\n",
        "    corr_matrix = df[num_cols + [target]].corr()\n",
        "    top_corr_cols = corr_matrix[target].abs().sort_values(ascending=False).iloc[1:11].index.tolist()\n",
        "    plt.figure(figsize=(10,6))\n",
        "    sns.heatmap(df[top_corr_cols + [target]].corr(), annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1)\n",
        "    plt.title('Matriz de Correlação - Top 10 numéricas')\n",
        "    plt.show()\n",
        "\n",
        "    # Valores ausentes\n",
        "    missing = (df.isna().sum()/len(df)*100).sort_values(ascending=False)\n",
        "    print(\"\\n=== Valores Ausentes (%) ===\")\n",
        "    display(missing[missing>0])\n",
        "\n",
        "    return df, num_cols\n",
        "\n",
        "df, num_cols = eda_regression(df, 'homicidio_doloso')\n",
        "\n",
        "# Aplicando Winsorização para outliers\n",
        "df = winsorize_outliers(df, num_cols)\n",
        "\n",
        "# ==============================================================\n",
        "# 7. Definição de features e target\n",
        "target = 'homicidio_doloso'\n",
        "\n",
        "# ==============================================================\n",
        "# 7.1 Remover colunas altamente correlacionadas\n",
        "corr_matrix = df[num_cols].corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "to_drop = [col for col in upper.columns if any(upper[col] > 0.9)]\n",
        "print(f\"Colunas removidas por alta correlação (>0.9): {to_drop}\")\n",
        "\n",
        "# Definir X removendo target e colunas altamente correlacionadas\n",
        "X = df.drop(columns=[target] + to_drop)\n",
        "y = df[target]\n",
        "\n",
        "# Atualizar colunas numéricas após remoção\n",
        "num_cols = [col for col in num_cols if col not in to_drop]\n",
        "\n",
        "# ==============================================================\n",
        "# 8. Divisão treino/teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "print(\"Treino:\", X_train.shape, \"Teste:\", X_test.shape)\n",
        "\n",
        "# ==============================================================\n",
        "# 9. Pré-processamento\n",
        "cat_cols = X_train.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "numeric_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy='median')),  # 1. Missing values numéricos\n",
        "    (\"scaler\", StandardScaler())                    # 2. Normalização\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy='most_frequent')),  # 1. Missing values categóricos\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown='ignore'))     # 3. OneHotEncoding\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", numeric_transformer, num_cols),\n",
        "    (\"cat\", categorical_transformer, cat_cols)\n",
        "])\n",
        "\n",
        "# ==============================================================\n",
        "# 10. Modelos candidatos\n",
        "models_params = {\n",
        "    \"LinearRegression\": {\"model\": LinearRegression(), \"params\": {}},\n",
        "    \"RandomForest\": {\"model\": RandomForestRegressor(random_state=SEED),\n",
        "                     \"params\": {\"model__n_estimators\": [50,100], \"model__max_depth\": [5,10]}},\n",
        "    \"GradientBoosting\": {\"model\": GradientBoostingRegressor(random_state=SEED),\n",
        "                         \"params\": {\"model__n_estimators\":[50,100], \"model__learning_rate\":[0.05,0.1], \"model__max_depth\":[3,5]}},\n",
        "    \"XGBoost\": {\"model\": XGBRegressor(random_state=SEED, verbosity=0),\n",
        "                \"params\": {\"model__n_estimators\":[50,100], \"model__learning_rate\":[0.05,0.1], \"model__max_depth\":[3,5]}},\n",
        "    \"LightGBM\": {\"model\": LGBMRegressor(random_state=SEED, verbose=-1),\n",
        "                 \"params\": {\"model__n_estimators\":[50,100], \"model__learning_rate\":[0.05,0.1], \"model__max_depth\":[5]}}\n",
        "}\n",
        "\n",
        "# ==============================================================\n",
        "# 11. Treino, RandomizedSearchCV e Avaliação\n",
        "best_models = {}\n",
        "metrics = []\n",
        "\n",
        "for name, mp in models_params.items():\n",
        "    print(f\"\\n=== Treino e Otimização {name} ===\")\n",
        "    pipe = Pipeline([(\"prep\", preprocessor), (\"model\", mp[\"model\"])])\n",
        "\n",
        "    if mp[\"params\"]:\n",
        "        search = RandomizedSearchCV(pipe, mp[\"params\"], n_iter=4, cv=3, scoring='r2', random_state=SEED, n_jobs=-1)\n",
        "        search.fit(X_train, y_train)\n",
        "        best_models[name] = search.best_estimator_\n",
        "        print(f\"Melhor R² CV: {search.best_score_:.3f}\")\n",
        "        print(f\"Melhores parâmetros: {search.best_params_}\")\n",
        "    else:\n",
        "        pipe.fit(X_train, y_train)\n",
        "        best_models[name] = pipe\n",
        "        score = cross_val_score(pipe, X_train, y_train, cv=3, scoring='r2').mean()\n",
        "        print(f\"R² CV: {score:.3f}\")\n",
        "\n",
        "    # Previsão e métricas\n",
        "    y_pred = best_models[name].predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    metrics.append({\"Model\": name, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2})\n",
        "\n",
        "# ==============================================================\n",
        "# 12. Tabela resumida de hiperparâmetros otimizados (após RandomizedSearchCV)\n",
        "hyperparams_summary = []\n",
        "\n",
        "for name, mp in models_params.items():\n",
        "    if mp[\"params\"]:  # Modelos que tiveram RandomizedSearchCV\n",
        "        best_params = best_models[name].named_steps[\"model\"].get_params()\n",
        "        filtered_params = {k: v for k, v in best_params.items() if k in [p.replace(\"model__\", \"\") for p in mp[\"params\"].keys()]}\n",
        "        hyperparams_summary.append({\"Model\": name, **filtered_params})\n",
        "    else:\n",
        "        hyperparams_summary.append({\"Model\": name, \"Info\": \"Sem hiperparâmetros ajustáveis\"})\n",
        "\n",
        "hyperparams_df = pd.DataFrame(hyperparams_summary)\n",
        "print(\"\\n=== Hiperparâmetros Otimizados ===\")\n",
        "display(hyperparams_df)\n",
        "\n",
        "# ==============================================================\n",
        "# 13. Comparativo final\n",
        "metrics_df = pd.DataFrame(metrics).sort_values(\"R2\", ascending=False)\n",
        "display(metrics_df)\n",
        "\n",
        "metrics_melt = metrics_df.melt(id_vars='Model', value_vars=['RMSE','MAE','R2'], var_name='Metric', value_name='Value')\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x='Model', y='Value', hue='Metric', data=metrics_melt)\n",
        "plt.title(\"Comparativo de Performance dos Modelos\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# ==============================================================\n",
        "# 14. Melhor modelo, análise de erros e Learning Curves\n",
        "best_name = metrics_df.iloc[0]['Model']\n",
        "best_model = best_models[best_name]\n",
        "print(f\"Melhor modelo selecionado: {best_name}\")\n",
        "\n",
        "# Real vs Predito\n",
        "y_pred = best_model.predict(X_test)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.3)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel(\"Real\")\n",
        "plt.ylabel(\"Predito\")\n",
        "plt.title(f\"Real vs Predito - {best_name}\")\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance (se aplicável)\n",
        "try:\n",
        "    model_obj = best_model.named_steps[\"model\"]\n",
        "    if hasattr(model_obj, \"feature_importances_\"):\n",
        "        cat_features = best_model.named_steps[\"prep\"].named_transformers_[\"cat\"].named_steps[\"onehot\"].get_feature_names_out(cat_cols)\n",
        "        feature_names = num_cols + list(cat_features)\n",
        "        importances = model_obj.feature_importances_\n",
        "        feat_imp = pd.DataFrame({\"feature\": feature_names, \"importance\": importances}).sort_values(\"importance\", ascending=False).head(20)\n",
        "        plt.figure(figsize=(10,6))\n",
        "        sns.barplot(x=\"importance\", y=\"feature\", data=feat_imp)\n",
        "        plt.title(f\"Top 20 Features - {best_name}\")\n",
        "        plt.show()\n",
        "except:\n",
        "    print(\"Não foi possível calcular feature importance para este modelo.\")\n",
        "\n",
        "# Learning Curves\n",
        "train_sizes, train_scores, test_scores = learning_curve(best_model, X_train, y_train, cv=3, scoring='r2', n_jobs=-1, train_sizes=np.linspace(0.1,1.0,5))\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(train_sizes, train_scores.mean(axis=1), \"o-\", label=\"Train R²\")\n",
        "plt.plot(train_sizes, test_scores.mean(axis=1), \"o-\", label=\"CV R²\")\n",
        "plt.xlabel(\"Amostra de Treino\")\n",
        "plt.ylabel(\"R²\")\n",
        "plt.title(f\"Learning Curve - {best_name}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gt_vxqZJxxzA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}